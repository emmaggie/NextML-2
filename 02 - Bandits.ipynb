{
 "metadata": {
  "language": "Julia",
  "name": "",
  "signature": "sha256:844c301c76e7a6a0a97d7e8fa211a214c8bd0e5f5cc7e3a5e2524fdccc7a9015"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Introducing the Multi-Armed Bandit Problem"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To introduce the bandit problem, imagine that you enter a casino and are shown two different slot machines. You're told that each machine pays out $500 on each win, but that one of the machines pays out wins more often than the other. You're then offered 100 free plays and are encouraged to try your best to win as much money as possible.\n",
      "\n",
      "How would you pick which machine to play on each of your 100 turns?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This problem is a very simple example of the multi-armed bandit problem, in which you're faced with a trade-off between taking actions that provide you with information you can use on subsequent plays and maximizing your chances of winning with your current play."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To get some familiarity with the problem, we're going to do Monte Carlo simulations and visualize the results. This means we'll need two packages loaded: Distributions and Gadfly."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "using Distributions\n",
      "using Gadfly"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Baseline"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As a simple baseline, let's consider selecting both arms with 50/50 odds:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "function total_rewards(p1, p2)\n",
      "    rewards = 0.0\n",
      "    for t in 1:100\n",
      "        if t <= 50\n",
      "            rewards += 500 * rand(Bernoulli(p1))\n",
      "        else\n",
      "            rewards += 500 * rand(Bernoulli(p2))\n",
      "        end\n",
      "    end\n",
      "    return rewards / 100\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can see what our average reward looks like by calling the function once:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "total_rewards(0.1, 0.2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To get a better sense of what the long-run average is after accounting for randomness in the outcomes, we'll need to do many more simulations:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "function simulate_rewards(n_sims, p1, p2)\n",
      "    rewards = Array(Float64, n_sims)\n",
      "    for sim in 1:n_sims\n",
      "        rewards[sim] = total_rewards(p1, p2)\n",
      "    end\n",
      "    return rewards\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rewards = simulate_rewards(25_000, 0.1, 0.2);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "With 25,000 simulations in hand, we can look at the distribution of rewards:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mean(rewards), std(rewards)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot(x = rewards, Geom.histogram)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As you would expect, our rewards are a 50/50 mixture of the two machine's average outcomes. Since our rewards are accumulated over many trials, the Central Limit Theorem kicks in and the resulting distribution looks Gaussian."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Strategically Exploring the Two Arms"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our approach so far to maximizing rewards above is pretty stupid. We gather a lot of information about which arm is better, but we don't use that information. We can do better by just sampling the two arms over two trials, then going with the best one from that point on:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "function total_rewards2(p1, p2)\n",
      "    rewards = 0.0\n",
      "    reward1 = 500 * rand(Bernoulli(p1))\n",
      "    reward2 = 500 * rand(Bernoulli(p2))\n",
      "    rewards += reward1 + reward2\n",
      "    if reward1 > reward2\n",
      "        preferred1 = true\n",
      "    elseif reward1 < reward2\n",
      "        preferred1 = false\n",
      "    else\n",
      "        if rand(Bernoulli(0.5)) == 0\n",
      "            preferred1 = true\n",
      "        else\n",
      "            preferred1 = false\n",
      "        end\n",
      "    end        \n",
      "    for t in 3:100\n",
      "        if preferred1\n",
      "            rewards += 500 * rand(Bernoulli(p1))\n",
      "        else\n",
      "            rewards += 500 * rand(Bernoulli(p2))\n",
      "        end\n",
      "    end\n",
      "    return rewards / 100\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "function simulate_rewards2(n_sims, p1, p2)\n",
      "    rewards = Array(Float64, n_sims)\n",
      "    for sim in 1:n_sims\n",
      "        rewards[sim] = total_rewards2(p1, p2)\n",
      "    end\n",
      "    return rewards\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rewards = simulate_rewards2(25_000, 0.1, 0.2);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can then look at the distribution of rewards to see how things work out:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mean(rewards), std(rewards)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot(x = rewards, Geom.histogram)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What happens here is interesting: we get higher rewards on average, but the variance of our outcomes has grown a lot. This isn't surprising: it's quite rare that our initial two samples tell us which arm is best. The rest of the time, we pick an arm at random -- which produces two bumps in our histogram."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can do better by learning about the best arm for more trials before picking a winner:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "function total_rewards3(p1, p2)\n",
      "    rewards = 0.0\n",
      "    reward1 = 0.0\n",
      "    reward2 = 0.0\n",
      "    for t_prime in 1:10\n",
      "        reward1 += 500 * rand(Bernoulli(p1))\n",
      "        reward2 += 500 * rand(Bernoulli(p2))\n",
      "    end\n",
      "    rewards += reward1 + reward2\n",
      "    if reward1 > reward2\n",
      "        preferred1 = true\n",
      "    elseif reward1 < reward2\n",
      "        preferred1 = false\n",
      "    else\n",
      "        if rand(Bernoulli(0.5)) == 0\n",
      "            preferred1 = true\n",
      "        else\n",
      "            preferred1 = false\n",
      "        end\n",
      "    end        \n",
      "    for t in 21:100\n",
      "        if preferred1\n",
      "            rewards += 500 * rand(Bernoulli(p1))\n",
      "        else\n",
      "            rewards += 500 * rand(Bernoulli(p2))\n",
      "        end\n",
      "    end\n",
      "    return rewards / 100\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "function simulate_rewards3(n_sims, p1, p2)\n",
      "    rewards = Array(Float64, n_sims)\n",
      "    for sim in 1:n_sims\n",
      "        rewards[sim] = total_rewards3(p1, p2)\n",
      "    end\n",
      "    return rewards\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rewards = simulate_rewards3(25_000, 0.1, 0.2);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mean(rewards), std(rewards)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot(x = rewards, Geom.histogram)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we see a much better outcome than either randomly making choices or doubling down on the arm that seems best in our first two trials. Let's try to find the truly best strategy by looping over all possible lengths of training periods:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "function total_rewards_train(p1, p2, train)\n",
      "    rewards = 0.0\n",
      "    reward1 = 0.0\n",
      "    reward2 = 0.0\n",
      "    for t_prime in 1:fld(train, 2)\n",
      "        reward1 += 500 * rand(Bernoulli(p1))\n",
      "        reward2 += 500 * rand(Bernoulli(p2))\n",
      "    end\n",
      "    rewards += reward1 + reward2\n",
      "    if reward1 > reward2\n",
      "        preferred1 = true\n",
      "    elseif reward1 < reward2\n",
      "        preferred1 = false\n",
      "    else\n",
      "        if rand(Bernoulli(0.5)) == 0\n",
      "            preferred1 = true\n",
      "        else\n",
      "            preferred1 = false\n",
      "        end\n",
      "    end        \n",
      "    for t in (train + 1):100\n",
      "        if preferred1\n",
      "            rewards += 500 * rand(Bernoulli(p1))\n",
      "        else\n",
      "            rewards += 500 * rand(Bernoulli(p2))\n",
      "        end\n",
      "    end\n",
      "    return rewards / 100\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "function simulate_rewards_train(n_sims, p1, p2, train)\n",
      "    rewards = Array(Float64, n_sims)\n",
      "    for sim in 1:n_sims\n",
      "        rewards[sim] = total_rewards_train(p1, p2, train)\n",
      "    end\n",
      "    return rewards\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "function reward_sampling_distribution_train(n_sims, p1, p2)\n",
      "    trains = 0:2:100\n",
      "    means = Array(Float64, length(trains))\n",
      "    stds = Array(Float64, length(trains))\n",
      "    for train in trains\n",
      "        rewards = simulate_rewards_train(25_000, p1, p2, train)\n",
      "        means[fld(train, 2) + 1] = mean(rewards)\n",
      "        stds[fld(train, 2) + 1] = std(rewards)\n",
      "    end\n",
      "    return means, stds\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "means, stds = reward_sampling_distribution_train(10_000, 0.1, 0.2);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot(x = 0:2:100, y = means, Geom.line)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot(x = 0:2:100, y = stds, Geom.line)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we see that the best average reward comes when we explore for 30 trials and then exploit what we've learned. But the variability in our outcomes is least when we don't try to use anything we've learned. This is a somewhat important result: trying to learn about the world makes our experiences more variable, not less."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Trade-Offs between Exploration and Exploitation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this example, we've explicitly partitioned our 100 trials into two periods: an exploration phasee and an exploitation phase. Importantly, we've seen that the highest mean reward occurs when we have an exploration phase that lasts for 28 trials."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This reflects a general pattern: in online optimization problems where we start off with imperfect information, we need to balance our need to acquire better information and our need to make use of the information we currently have.\n",
      "\n",
      "This tradeoff is called the explore-exploit dilemma. Let's consider some formal algorithms people have used to study the dilemma."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Defining the Arms in a Multi-Armed Bandit Problem"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First, we'll define a type to represent a single arm in a multi-armed bandit problem. Since we don't have a lot of time, we'll just use arms that produce 0/1 outputs with some probability p:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "abstract Arm"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "immutable BernoulliArm <: Arm\n",
      "    p::Float64\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "draw(arm::BernoulliArm) = rand(Bernoulli(arm.p))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Generic Code to Learn the Means of the Arms"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, we'll set up a type that we can plug into every algorithm to learn the mean of each arm:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "abstract Learner"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "immutable MeanLearner <: Learner\n",
      "    ns::Vector{Int64}\n",
      "    \u03bcs::Vector{Float64}\n",
      "    \u03bc\u2080::Float64\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "function MeanLearner(\u03bc\u2080::Real = 0.0)\n",
      "    return MeanLearner(\n",
      "      Array(Int64, 0),\n",
      "      Array(Float64, 0),\n",
      "      float64(\u03bc\u2080),\n",
      "    )\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "function initialize!(learner::MeanLearner, K::Integer)\n",
      "    resize!(learner.ns, K)\n",
      "    resize!(learner.\u03bcs, K)\n",
      "\n",
      "    fill!(learner.ns, 0)\n",
      "    fill!(learner.\u03bcs, learner.\u03bc\u2080)\n",
      "\n",
      "    return\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "function learn!(learner::MeanLearner, i::Integer, r_t::Real)\n",
      "    learner.ns[i] += 1\n",
      "    n\u1d62 = learner.ns[i]\n",
      "\n",
      "    if n\u1d62 == 1\n",
      "        learner.\u03bcs[i] = r_t\n",
      "    else\n",
      "        learner.\u03bcs[i] = (1 - 1 / n\u1d62) * learner.\u03bcs[i] + (1 / n\u1d62) * r_t\n",
      "    end\n",
      "\n",
      "    return\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Bandit Algorithms"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, let's set up a type to represent an algorithm for solving the problem:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "abstract Algorithm"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "function initialize!(algorithm::Algorithm, K::Integer)\n",
      "    initialize!(algorithm.learner, K)\n",
      "\n",
      "    return\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "immutable EpsilonGreedy{L <: Learner} <: Algorithm\n",
      "    learner::L\n",
      "    \u03b5::Float64\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "function Base.select(algorithm::EpsilonGreedy, K::Integer, t::Integer)\n",
      "    if rand() < algorithm.\u03b5\n",
      "        return rand(1:K)\n",
      "    else\n",
      "        return indmax(algorithm.learner.\u03bcs)\n",
      "    end\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Simulations"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have the fundamental components of a bandit algorithm built out, we need to define a function to do Monte Carlo simulations to see how the algorithm fares in practice:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "function regret(arms, r_t)\n",
      "    max_gap = -Inf\n",
      "    for arm in arms\n",
      "        gap = arm.p - r_t\n",
      "        if gap > max_gap\n",
      "            max_gap = gap\n",
      "        end\n",
      "    end\n",
      "    return max_gap\n",
      "end "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "function simulate(T::Integer, S::Integer, algorithm::Algorithm, arms::Vector)\n",
      "    K = length(arms)\n",
      "    cumulative_regrets = Array(Float64, S)\n",
      "    for s in 1:S\n",
      "        initialize!(algorithm, K)\n",
      "        cumulative_regret = 0.0\n",
      "        for t in 1:T\n",
      "            a = select(algorithm, K, t)\n",
      "            r_t = draw(arms[a])\n",
      "            learn!(algorithm.learner, a, r_t)\n",
      "            immediate_regret = regret(arms, r_t)\n",
      "            cumulative_regret += immediate_regret\n",
      "        end\n",
      "        cumulative_regrets[s] = cumulative_regret\n",
      "    end\n",
      "\n",
      "    return mean(cumulative_regrets)\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "simulate(1000, 25_000, EpsilonGreedy(MeanLearner(), 0.1), [BernoulliArm(0.1), BernoulliArm(0.2)])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "UCB1"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The algorithm we've just shown is pretty simplistic. A better algorithm is UCB1, shown below. It's a very useful exercise to think about why UCB1 should be better than the Epsilon-Greedy algorithm."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "immutable UCB1{L <: Learner} <: Algorithm\n",
      "    learner::L\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "function initialize!(algorithm::UCB1, K::Integer)\n",
      "    initialize!(algorithm.learner, K)\n",
      "    return\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "function Base.select(algorithm::UCB1, K::Integer, t::Integer)\n",
      "    \u03bcs = algorithm.learner.\u03bcs\n",
      "    ns = algorithm.learner.ns\n",
      "\n",
      "    for i in 1:K\n",
      "        if ns[i] == 0\n",
      "            return i\n",
      "        end\n",
      "    end\n",
      "\n",
      "    max_score, c = -Inf, 0\n",
      "    for i in 1:K\n",
      "        bonus = sqrt(2 * log(t) / ns[i])\n",
      "        score = \u03bcs[i] + bonus\n",
      "        if score > max_score\n",
      "            max_score = score\n",
      "            c = i\n",
      "        end\n",
      "    end\n",
      "    return c\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "simulate(1000, 10_000, EpsilonGreedy(MeanLearner(), 0.1), [BernoulliArm(0.01), BernoulliArm(0.02)])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "simulate(1000, 10_000, UCB1(MeanLearner()), [BernoulliArm(0.01), BernoulliArm(0.02)])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}